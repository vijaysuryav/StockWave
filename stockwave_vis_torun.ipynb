{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82ffac5-86cc-4fd9-93b3-46be5bca8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a4e5c6-18e4-4ca3-a0bd-370ac4731187",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [file for file in os.listdir('data_torun') if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab239de-28c6-46a5-b625-402d2b8d57ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHD_data.csv',\n",
       " 'CVX_data.csv',\n",
       " 'DUK_data.csv',\n",
       " 'DVA_data.csv',\n",
       " 'EQR_data.csv',\n",
       " 'ESS_data.csv',\n",
       " 'FOX_data.csv',\n",
       " 'GILD_data.csv',\n",
       " 'GPS_data.csv',\n",
       " 'HAL_data.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ce739a-fc14-4f7a-9718-7875064611fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 22:34:35.078604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 22:34:37.208603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2abfc1bc-9ed8-4bcd-b564-38c812dbd006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scaling(arr, mean, std):\n",
    "    return arr * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f33977",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [file.split('_')[0] for file in os.listdir('data_torun') if file.endswith('.csv')]\n",
    "test_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732b3cf1-68db-47e2-ae8b-1a15ec5b4169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHD', 'CVX', 'DUK', 'DVA', 'EQR', 'ESS', 'FOX', 'GILD', 'GPS', 'HAL']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554b529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mean and std dictionaries from the working directory\n",
    "with open('means_torun/means_dict_torun.json', 'r') as f:\n",
    "    means_dict = json.load(f)\n",
    "\n",
    "with open('stds_torun/stds_dict_torun.json', 'r') as f:\n",
    "    std_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ae114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average (EMA)\n",
    "def add_ema(df, alpha=0.2):\n",
    "    df['EMA'] = df['close'].ewm(alpha=alpha, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "# Relative Strength Index (RSI)\n",
    "def add_rsi(df, window=14):\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cefaff-88c1-4cf0-b345-bf8bc5a3c082",
   "metadata": {},
   "source": [
    "### GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0487755b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b458707affa541b69eeaf46a142b99d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Stock:', options=('CHD', 'CVX', 'DUK', 'DVA', 'EQR', 'ESS', 'FOX',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Load mean and std dictionaries from the working directory\n",
    "with open('means_torun/means_dict_torun.json', 'r') as f:\n",
    "    means_dict = json.load(f)\n",
    "\n",
    "with open('stds_torun/stds_dict_torun.json', 'r') as f:\n",
    "    std_dict = json.load(f)\n",
    "    \n",
    "def df_to_X_y2(df, window_size=5):\n",
    "    df_as_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df_as_np)-window_size):\n",
    "        row = [r for r in df_as_np[i:i+window_size]]\n",
    "        X.append(row)\n",
    "        label = df_as_np[i+window_size][3]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# to standardisze the test data\n",
    "def preprocess_X(X, means, stds):\n",
    "    X[:, :, 0] = (X[:, :, 0] - means['open']) / stds['open']\n",
    "    X[:, :, 1] = (X[:, :, 1] - means['high']) / stds['high']\n",
    "    X[:, :, 2] = (X[:, :, 2] - means['low']) / stds['low']\n",
    "    X[:, :, 3] = (X[:, :, 3] - means['close']) / stds['close']\n",
    "    X[:, :, 4] = (X[:, :, 4] - means['EMA']) / stds['EMA']\n",
    "    X[:, :, 5] = (X[:, :, 5] - means['RSI']) / stds['RSI']\n",
    "    return X\n",
    "\n",
    "# standardise y in case we wish to train\n",
    "def preprocess_y(y, mean, std):\n",
    "    y[:] = (y[:] - mean) / std\n",
    "    return y\n",
    "\n",
    "# Load data and preprocess\n",
    "def load_data(stock_name):\n",
    "    df = pd.read_csv(f'data_test_torun/{stock_name}_data.csv') \n",
    "    df['open'] = df['open'].astype(float)\n",
    "    df['high'] = df['high'].astype(float)\n",
    "    df['close'] = df['close'].astype(float)\n",
    "    df['low'] = df['low'].astype(float)\n",
    "    df['EMA'] = df['close'].ewm(alpha=0.3, min_periods=1).mean()\n",
    "    \n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=14, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    return df\n",
    "\n",
    "# filtering data from the previous dates\n",
    "def prepare_data(df, target_date, means, stds):\n",
    "    subset_df =df[df['date'] == str(target_date)]\n",
    "    target_index = subset_df.index[0]\n",
    "    if not np.isnan(target_index):  # Check if target_index is not NaN\n",
    "        df_subset = df.iloc[target_index-50:target_index].copy()  # Adjust range to prevent negative indices\n",
    "        df_subset.drop(columns=['date','Name','volume'],inplace =True)\n",
    "        X, y = df_to_X_y2(df_subset)\n",
    "        X = preprocess_X(X, means, stds)\n",
    "        y = preprocess_y(y, means['close'], stds['close'])\n",
    "        return X, y\n",
    "    else:\n",
    "        print(f\"Date {target_date} not found in the dataframe.\")\n",
    "        return None, None\n",
    "\n",
    "#  import model and predict closing values\n",
    "def predict_closing(model_path, X):\n",
    "    model = load_model(model_path)\n",
    "    y_pred = model.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "# invert transform the predicted \n",
    "def postprocess(y,mean,std):\n",
    "    y = y*std +mean\n",
    "    return y\n",
    "\n",
    "\n",
    "# online training \n",
    "def retrain(stock_name, target_date):\n",
    "    df = load_data(stock_name)\n",
    "    df = df[df['date'] <= str(target_date)]  \n",
    "    # prepare data using only data before the selected date\n",
    "    X, y = prepare_data(df, target_date, means_dict[stock_name], std_dict[stock_name])\n",
    "    \n",
    "    model_path = f'modelstorun_/{stock_name}_model.keras'\n",
    "    model = load_model(model_path)\n",
    " # Train model using only the data before the selected date\n",
    "    cp = ModelCheckpoint(f'models_onlinetrained/{stock_name}_model.keras', save_best_only=True)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=MeanSquaredError(), metrics=[RootMeanSquaredError()])\n",
    "    model.fit(X, y, epochs=100,  callbacks = [cp],verbose= 0 )\n",
    "    model.save(f\"models_onlinetrained/{stock_name}_model.keras\")\n",
    "#     trained_models[stock_name] = model\n",
    "    # Predict closing values\n",
    "#     y_pred = model.predict(X)\n",
    "\n",
    "\n",
    "# visualize predictions and actual values\n",
    "def visualize(stock_name, df, target_date):\n",
    "    X, y = prepare_data(df, target_date, means_dict[stock_name], std_dict[stock_name])\n",
    "    \n",
    "    \n",
    "    trained_model_path = f'models_onlinetrained/{stock_name}_model.keras'\n",
    "    if os.path.exists(trained_model_path):\n",
    "        model_path = trained_model_path\n",
    "        print('taking trained model')\n",
    "    else:\n",
    "        model_path = f'modelstorun_/{stock_name}_model.keras'\n",
    "        print('taking original model')\n",
    "    y_pred = predict_closing(model_path, X)\n",
    "    \n",
    "  # getting the last 45 dates from the target date\n",
    "    target_index = df[df['date'] == str(target_date)].index[0]\n",
    "    start_index = max(0, target_index - 44)  # Adjusted to get 45 dates\n",
    "    end_index = target_index + 1\n",
    "    subset_df = df.iloc[start_index:end_index]\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(subset_df['date'], postprocess(y, means_dict[stock_name]['close'], std_dict[stock_name]['close']), label='Actual', color='blue')\n",
    "    ax.plot(subset_df['date'], postprocess(y_pred.flatten(), means_dict[stock_name]['close'], std_dict[stock_name]['close']), label='Predicted', color='red')\n",
    "    \n",
    "    # Formatting x-axis date labels\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    ax.set_title(\"Predictions vs Actual Closing Values\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Closing Value\")\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# GUI function\n",
    "def visualize_stock():\n",
    "    stock_dropdown = widgets.Dropdown(options=test_list[:10],\n",
    "                                      description='Select Stock:')\n",
    "#     date_input = widgets.DatePicker(description='Select Date:')\n",
    "    date_input = widgets.DatePicker(description='Select Date:') #, min=pd.Timestamp('2020-02-05'), max=pd.Timestamp('2023-12-29'))\n",
    "    visualize_button = widgets.Button(description=\"Visualize\")\n",
    "    clear_button = widgets.Button(description=\"Clear Graph\")\n",
    "    output_graph = widgets.Output()\n",
    "    train_button = widgets.Button(description = \"Retrain\")\n",
    "    def on_visualize_button_clicked(b):\n",
    "        stock_name = stock_dropdown.value\n",
    "        target_date = date_input.value\n",
    "        df = load_data(stock_name)\n",
    "        with output_graph:\n",
    "            visualize(stock_name, df, target_date)\n",
    "            \n",
    "    def on_clear_button_clicked(b):\n",
    "        output_graph.clear_output()\n",
    "        \n",
    "        \n",
    "    def on_train_button_clicked(b):\n",
    "        stock_name = stock_dropdown.value\n",
    "        target_date = date_input.value\n",
    "        with output_graph:\n",
    "            retrain(stock_name, target_date)\n",
    "            \n",
    "            \n",
    "    visualize_button.on_click(on_visualize_button_clicked)\n",
    "    clear_button.on_click(on_clear_button_clicked)\n",
    "    train_button.on_click(on_train_button_clicked)\n",
    "    display(widgets.VBox([stock_dropdown, date_input, visualize_button,clear_button,train_button, output_graph]))\n",
    "\n",
    "# Run GUI\n",
    "visualize_stock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb8f70-d1aa-4fb8-9e0e-97a6caf3fa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5e063-6ab0-4dcd-af79-c6fd606fa1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
